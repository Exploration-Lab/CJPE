{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN _final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0LeFv-AvMWDb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d8bb93c-ec58-442f-91a2-dee32706b6c6"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import load\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano' # Using theano as backend instead of tensorflow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Input\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Theano backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D9hc66hBfxtO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9fdc2de8-4872-446e-9f58-ac290b4ad1a8"
      },
      "source": [
        "# Mounting google drive, depends wherever the data is\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxqOhOHhhnZx",
        "colab": {}
      },
      "source": [
        "# path to dataset\n",
        "path_to_dataset = 'drive/My Drive/dataset.csv'\n",
        "# path to transformer generated chunk embeddings eg. XLNet etc.\n",
        "path_to_transformer_chunk_embeddings_train = 'drive/My Drive/XLNet/XLNet_train.npy'\n",
        "path_to_transformer_chunk_embeddings_dev = 'drive/My Drive/XLNet/XLNet_dev.npy'\n",
        "path_to_transformer_chunk_embeddings_test = 'drive/My Drive/XLNet/XLNet_test.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRSi5B_5hcZJ",
        "colab": {}
      },
      "source": [
        "# loading dataset\n",
        "dataset = pd.read_csv(path_to_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09DRYpIrhrpJ",
        "colab": {}
      },
      "source": [
        "# loading the chunks embeddings\n",
        "x_train0 = load(path_to_transformer_chunk_embeddings_train, allow_pickle = True)\n",
        "x_dev0 = load(path_to_transformer_chunk_embeddings_dev, allow_pickle= True)\n",
        "x_test0 = load(path_to_transformer_chunk_embeddings_test, allow_pickle= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y2ORii42h_81",
        "colab": {}
      },
      "source": [
        "# loading the corresponding label for each case in dataset\n",
        "dev = dataset.loc[dataset['split'] == 'dev']\n",
        "train = dataset.loc[dataset['split'] == 'train']\n",
        "test = dataset.loc[dataset['split'] == 'test']\n",
        "\n",
        "y_train0 = []\n",
        "for i in range(train.shape[0]):\n",
        "  y_train0.append(train.loc[i,'label'])\n",
        "\n",
        "y_dev0 = []\n",
        "for i in range(dev.shape[0]):\n",
        "  y_dev0.append(dev.loc[i+32305,'label'])\n",
        "\n",
        "y_test0 = []\n",
        "for i in range(test.shape[0]):\n",
        "  y_test0.append(test.loc[i+33299,'label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ybuEL6kaNs9C",
        "colab": {}
      },
      "source": [
        "# Setting the maximum sequnce length and embedding dimension\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mwmq8C42iPv9",
        "colab": {}
      },
      "source": [
        "# padding the vectors to maximum sequence length\n",
        "for i in range(x_train0.shape[0]):\n",
        "  padding_vector = np.zeros((MAX_SEQUENCE_LENGTH - x_train0[i].shape[0], EMBEDDING_DIM))\n",
        "  x_train0[i] = np.concatenate((x_train0[i],padding_vector), axis = 0)\n",
        "    \n",
        "for i in range(x_dev0.shape[0]):\n",
        "  padding_vector = np.zeros((MAX_SEQUENCE_LENGTH - x_dev0[i].shape[0], EMBEDDING_DIM))\n",
        "  x_dev0[i] = np.concatenate((x_dev0[i],padding_vector),axis = 0)\n",
        "    \n",
        "for i in range(x_test0.shape[0]):\n",
        "  padding_vector = np.zeros((MAX_SEQUENCE_LENGTH - x_test0[i].shape[0], EMBEDDING_DIM))\n",
        "  x_test0[i] = np.concatenate((x_test0[i],padding_vector), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhP_WJKxNLBM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "2ae271f3-0203-450b-f9d0-fa69d2ea3063"
      },
      "source": [
        "# Using Input layer to convert into required tensor shape\n",
        "text_input = Input(shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), dtype='float32', name='text')\n",
        "# Using 3 Conv1D layers followed by max pooling layers\n",
        "l_cov1= Conv1D(256, 2, activation='relu')(text_input)\n",
        "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
        "l_cov2 = Conv1D(128, 2, activation='relu')(l_pool1)\n",
        "l_pool2 = MaxPooling1D(2)(l_cov2)\n",
        "l_cov3 = Conv1D(128, 2, activation='relu')(l_pool2)\n",
        "l_pool3 = MaxPooling1D(4)(l_cov3)  # global max pooling\n",
        "# Using the flatten layer to convert into 1D tensor\n",
        "l_flat = Flatten()(l_pool3)\n",
        "# passing the output embeddings through 2 dense layers\n",
        "l_dense = Dense(128, activation='relu')(l_flat)\n",
        "l_dense1 = Dense(32, activation='relu')(l_dense)\n",
        "# Using sigmoid classifier\n",
        "preds = Dense(1, activation='sigmoid')(l_dense1)\n",
        "\n",
        "model = Model(text_input, preds)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"Simplified convolutional neural network\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simplified convolutional neural network\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text (InputLayer)            (None, 25, 768)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 24, 256)           393472    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 12, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 11, 128)           65664     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 4, 128)            32896     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 512,705\n",
            "Trainable params: 512,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "reXHxrJIUEs4",
        "colab": {}
      },
      "source": [
        "num_sequences = len(x_train0)\n",
        "batch_size = 32\n",
        "batches_per_epoch =  int(num_sequences/batch_size)\n",
        "num_features= 768\n",
        "def train_generator(): # function to generate batches of corresponding batch size\n",
        "    x_list= x_train0\n",
        "    y_list =  y_train0\n",
        "    # Generate batches\n",
        "    while True:\n",
        "        for b in range(batches_per_epoch):\n",
        "            longest_index = (b + 1) * batch_size - 1\n",
        "            timesteps = len(max(x_train0[:(b + 1) * batch_size][-batch_size:], key=len))\n",
        "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
        "            y_train = np.zeros((batch_size,  1))\n",
        "            for i in range(batch_size):\n",
        "                li = b * batch_size + i\n",
        "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
        "                y_train[i] = y_list[li]\n",
        "            yield x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XsuS57oHfUrJ",
        "colab": {}
      },
      "source": [
        "num_sequences_val = len(x_dev0)\n",
        "batch_size_val = 32\n",
        "batches_per_epoch_val = int(num_sequences_val/batch_size_val)\n",
        "num_features= 768\n",
        "def val_generator(): # function to generate batches of corresponding batch size\n",
        "    x_list= x_dev0\n",
        "    y_list =  y_dev0\n",
        "    # Generate batches\n",
        "    while True:\n",
        "        for b in range(batches_per_epoch_val):\n",
        "            longest_index = (b + 1) * batch_size_val - 1\n",
        "            timesteps = len(max(x_dev0[:(b + 1) * batch_size_val][-batch_size_val:], key=len))\n",
        "            x_train = np.full((batch_size_val, timesteps, num_features), 0)\n",
        "            y_train = np.zeros((batch_size_val,  1))\n",
        "            for i in range(batch_size_val):\n",
        "                li = b * batch_size_val + i\n",
        "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
        "                y_train[i] = y_list[li]\n",
        "            yield x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1v2gE2HmeRj",
        "colab": {}
      },
      "source": [
        "num_features= 768\n",
        "def test_generator(): # function to generate batches of corresponding batch size\n",
        "    x_list= x_test0\n",
        "    y_list =  y_test0\n",
        "    # Generate batches\n",
        "    while True:\n",
        "        for b in range(batches_per_epoch_test):\n",
        "            if(b == batches_per_epoch_test-1): # An extra if else statement just to manage the last batch as it's size might not be equal to batch size \n",
        "              longest_index = num_sequences_test - 1\n",
        "              timesteps = len(max(x_test0[:longest_index + 1][-batch_size_test:], key=len))\n",
        "              x_train = np.full((longest_index - b*batch_size_test, timesteps, num_features), -99.)\n",
        "              y_train = np.zeros((longest_index - b*batch_size_test,  1))\n",
        "              for i in range(longest_index - b*batch_size_test):\n",
        "                  li = b * batch_size_test + i\n",
        "                  x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
        "                  y_train[i] = y_list[li]\n",
        "            else:\n",
        "                longest_index = (b + 1) * batch_size_test - 1\n",
        "                timesteps = len(max(x_test0[:(b + 1) * batch_size_test][-batch_size_test:], key=len))\n",
        "                x_train = np.full((batch_size_test, timesteps, num_features), -99.)\n",
        "                y_train = np.zeros((batch_size_test,  1))\n",
        "                for i in range(batch_size_test):\n",
        "                    li = b * batch_size_test + i\n",
        "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
        "                    y_train[i] = y_list[li]\n",
        "            yield x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YuuHo0_HfbFf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "9d36946e-e46e-49b5-90b4-004dc249702e"
      },
      "source": [
        "# Setting the callback and training the model\n",
        "call_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=2, verbose=2,\n",
        "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)\n",
        "\n",
        "model.fit_generator(train_generator(), steps_per_epoch=batches_per_epoch, epochs=3,\n",
        "                    validation_data=val_generator(), validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1009/1009 [==============================] - 130s 129ms/step - loss: 0.4970 - acc: 0.7544 - val_loss: 0.4926 - val_acc: 0.7772\n",
            "Epoch 2/3\n",
            "1009/1009 [==============================] - 137s 136ms/step - loss: 0.4724 - acc: 0.7691 - val_loss: 0.5014 - val_acc: 0.7762\n",
            "Epoch 3/3\n",
            "1009/1009 [==============================] - 142s 141ms/step - loss: 0.4584 - acc: 0.7793 - val_loss: 0.4891 - val_acc: 0.7853\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5529f1cb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E0LgRWkdjYFm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77ce0a57-3499-4487-ee29-fe8f40ce58be"
      },
      "source": [
        "num_sequences_test = len(x_test0)\n",
        "batch_size_test = 32\n",
        "batches_per_epoch_test = int(num_sequences_test/batch_size_test) + 1\n",
        "num_features= 768\n",
        "# evaluating on the test data\n",
        "model.evaluate_generator(test_generator(), steps= batches_per_epoch_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.548132598400116, 0.7691292762756348]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TAUfLPHHBfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining a function which calculates various metrics such as micro and macro precision, accuracy and f1\n",
        "def metrics_calculator(preds, test_labels):\n",
        "    cm = confusion_matrix(test_labels, preds)\n",
        "    TP = []\n",
        "    FP = []\n",
        "    FN = []\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[i][j]\n",
        "\n",
        "        FN.append(summ)\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[j][i]\n",
        "\n",
        "        FP.append(summ)\n",
        "    for i in range(0,2):\n",
        "        TP.append(cm[i][i])\n",
        "    precision = []\n",
        "    recall = []\n",
        "    for i in range(0,2):\n",
        "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
        "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
        "\n",
        "    macro_precision = sum(precision)/2\n",
        "    macro_recall = sum(recall)/2\n",
        "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
        "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
        "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
        "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
        "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aLiyuj2xmc64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6455f0a-e906-4ab9-eb01-cfcd5f699b18"
      },
      "source": [
        "# getting the predicted labels on the test data\n",
        "preds = model.predict_generator(test_generator(), steps= batches_per_epoch_test)\n",
        "y_pred = preds > 0.5\n",
        "\n",
        "# Calculating all metrics on test data predicted label\n",
        "print(metrics_calculator(y_pred, y_test0[:-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.7789586641591649, 0.7687549494826431, 0.7738231714660547, 0.7691292875989446, 0.7691292875989446, 0.7691292875989446)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwHXW9Anmw2y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8d76501-0466-4ca4-aa82-272de0dcdb99"
      },
      "source": [
        "# getting the predicted labels on the dev data\n",
        "preds = model.predict_generator(val_generator(), steps= batches_per_epoch_val)\n",
        "y_pred_dev = preds > 0.5\n",
        "\n",
        "# Calculating all metrics on dev data predicted label\n",
        "print(metrics_calculator(y_pred_dev, y_dev0[:-2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.7864010120177103, 0.7852822580645161, 0.7858412368659745, 0.7852822580645161, 0.7852822580645161, 0.7852822580645161)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9N9ZqI1rm1Gd",
        "colab": {}
      },
      "source": [
        "# saving the trained model\n",
        "model.save('CNN_XLNet.h5')  # creates a HDF5 file 'CNN_XLNet.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zB5C5WGlrJ5i",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}