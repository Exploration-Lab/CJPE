{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q1VFvx4I00a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6e9a6ef0-f08d-45b1-d2b4-111c9a710542"
      },
      "source": [
        "import nltk\n",
        "import progressbar\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import sys\n",
        "import keras\n",
        "from keras.layers import (\n",
        "    Dense, GRU, TimeDistributed, Input,\n",
        "    Embedding, Bidirectional, Lambda\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import _pickle as cPickle\n",
        "from keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICVOqMQjTf_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f5b432a6-ea10-480d-d248-832ba855b1fa"
      },
      "source": [
        "# Mounting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbHd06FIVPIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_dataset = \"drive/My Drive/dataset.csv\" # path to dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nCWQtTfPRfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(path_dataset) # loading dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHEYWa8QxkZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_WORDS_PER_SENT = 50 # maximum limit of words per sentence\n",
        "MAX_SENT = 40 # maximum number of sentences taken for every case from the end\n",
        "MAX_VOC_SIZE = 100000 # max vocabularity size\n",
        "GLOVE_DIM = 180 # dimension of glove embeddings we are feeding to network"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeKiR733yBIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset2 = dataset # making a copy of the dataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5rSLvWvgzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_quotations(text): # function to remove quotations\n",
        "    \"\"\"\n",
        "    Remove quotations and slashes from the dataset.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\\\\", \"\", text)\n",
        "    text = re.sub(r\"\\'\", \"\", text)\n",
        "    text = re.sub(r\"\\\"\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAYFzYwdx3lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset2['text'] = dataset2['text'].apply(remove_quotations) # removing quotations from dataset\n",
        "dataset2['text'] = dataset2['text'].apply(lambda x: x.strip().lower()) # converting data to lowr case"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7c9es60yH-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting data into train,test and val\n",
        "train = dataset2.loc[dataset2['split'] == 'train']\n",
        "test = dataset2.loc[dataset2['split'] == 'test']\n",
        "val = dataset2.loc[dataset2['split'] == 'dev']\n",
        "text = dataset2['text'].values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiEF-g23yOSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_tokenizer = Tokenizer(num_words=MAX_VOC_SIZE) # creating a word tokenizer\n",
        "word_tokenizer.fit_on_texts(text) # fitting the tokenizer on out text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmQW7NAo1Q8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the labels and the x values for train,test and val\n",
        "x_train = train['text'].values\n",
        "y_train = train['label'].values\n",
        "x_val = val['text'].values\n",
        "y_val = val['label'].values\n",
        "x_test = test['text'].values\n",
        "y_test = test['label'].values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqiYnkwN0Nbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad6dd275-3b6b-4057-98de-c598f34d4188"
      },
      "source": [
        "# tokenizing the cases and taking MAX_SENT sentences from each case with atmost MAX_WORDS_PER_SENT tokens for train data\n",
        "X_train = np.zeros((len(x_train), MAX_SENT, MAX_WORDS_PER_SENT), dtype='int32')\n",
        "for i, text in progressbar.progressbar(enumerate(x_train)):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = word_tokenizer.texts_to_sequences(\n",
        "        sentences\n",
        "    )\n",
        "    tokenized_sentences = pad_sequences(\n",
        "        tokenized_sentences, maxlen=MAX_WORDS_PER_SENT\n",
        "    )\n",
        "\n",
        "    pad_size = MAX_SENT - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size < 0:\n",
        "        tokenized_sentences = tokenized_sentences[0:MAX_SENT]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0,pad_size),(0,0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "\n",
        "    # Store this observation as the i-th observation in\n",
        "    # the data matrix\n",
        "    X_train[i] = tokenized_sentences[None, ...]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| |   #                                           | 32304 Elapsed Time: 0:04:51\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2DCcyJd16nt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5653625b-ff47-4eb2-bb1d-b9625ae946dd"
      },
      "source": [
        "# tokenizing the cases and taking MAX_SENT sentences from each case with atmost MAX_WORDS_PER_SENT tokens for validation data\n",
        "X_val = np.zeros((len(x_val), MAX_SENT, MAX_WORDS_PER_SENT), dtype='int32')\n",
        "for i, text in progressbar.progressbar(enumerate(x_val)):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = word_tokenizer.texts_to_sequences(\n",
        "        sentences\n",
        "    )\n",
        "    tokenized_sentences = pad_sequences(\n",
        "        tokenized_sentences, maxlen=MAX_WORDS_PER_SENT\n",
        "    )\n",
        "\n",
        "    pad_size = MAX_SENT - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size < 0:\n",
        "        tokenized_sentences = tokenized_sentences[0:MAX_SENT]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0,pad_size),(0,0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "\n",
        "    # Store this observation as the i-th observation in\n",
        "    # the data matrix\n",
        "    X_val[i] = tokenized_sentences[None, ...]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| |         #                                       | 993 Elapsed Time: 0:00:10\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYWbwZQaYZcU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cc21264-9d9c-4c4c-e7bb-cb009847bd8a"
      },
      "source": [
        "# tokenizing the cases and taking MAX_SENT sentences from each case with atmost MAX_WORDS_PER_SENT tokens for test data\n",
        "x_test_check = []\n",
        "X_test = np.zeros((len(x_test), MAX_SENT, MAX_WORDS_PER_SENT), dtype='int32')\n",
        "for i, text in progressbar.progressbar(enumerate(x_test)):\n",
        "    sentences = sent_tokenize(text)\n",
        "    x_test_check.append(sentences[max(-40,-len(sentences)):])\n",
        "    tokenized_sentences = word_tokenizer.texts_to_sequences(\n",
        "        sentences\n",
        "    )\n",
        "    tokenized_sentences = pad_sequences(\n",
        "        tokenized_sentences, maxlen=MAX_WORDS_PER_SENT\n",
        "    )\n",
        "\n",
        "    pad_size = MAX_SENT - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size < 0:\n",
        "        tokenized_sentences = tokenized_sentences[0:MAX_SENT]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0,pad_size),(0,0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "\n",
        "    # Store this observation as the i-th observation in\n",
        "    # the data matrix\n",
        "    X_test[i] = tokenized_sentences[None, ...]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| |                           #                    | 1516 Elapsed Time: 0:00:16\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8YJv9fr2GzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting labels into categorical format\n",
        "Y_train = to_categorical(y_train)\n",
        "Y_val = to_categorical(y_val)\n",
        "Y_test = to_categorical(y_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOtslbiD2WZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the glove embeddings(trained on our dataset)\n",
        "path_glove_pretrained_embeddings = 'drive/My Drive/Glove_model/glove_epoch20.model'\n",
        "with open(path_glove_pretrained_embeddings, 'rb') as f:\n",
        "  model = cPickle.load(f)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD788fzb2ocu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initializing the embedding matrix\n",
        "embedding_matrix = np.random.random(\n",
        "    (len(word_tokenizer.word_index) + 1, GLOVE_DIM)\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErBUftdj3Bte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting the embedding of pad token\n",
        "embedding_matrix[0] = 0"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxozYWZ15hao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# deleting dataset to free space as not required now\n",
        "del dataset\n",
        "del dataset2"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDgDZITH3_A8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the embedding matrix\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "  ind = model['dictionary'].get(word)\n",
        "  if ind is not None:\n",
        "    embedding_vector = model['word_vectors'][ind]\n",
        "    embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYVQAUF-6YCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25eada31-e6cc-410a-b522-030c8df85f24"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(299496, 180)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7XeqPs8609y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the attention layer for our HAN model\n",
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self, context_vector_length=100, **kwargs):\n",
        "        \"\"\"\n",
        "        An implementation of a attention layer. This layer\n",
        "        accepts a 3d Tensor (batch_size, time_steps, input_dim) and\n",
        "        applies a single layer attention mechanism in the time\n",
        "        direction (the second axis).\n",
        "        :param context_vector_lenght: (int) The size of the hidden context vector.\n",
        "            If set to 1 this layer reduces to a standard attention layer.\n",
        "        :param kwargs: Any argument that the baseclass Layer accepts.\n",
        "        \"\"\"\n",
        "        self.context_vector_length = context_vector_length\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[2]\n",
        "\n",
        "        # Add a weights layer for the\n",
        "        self.W = self.add_weight(\n",
        "            name='W', shape=(dim, self.context_vector_length),\n",
        "            initializer=keras.initializers.get('uniform'),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        self.u = self.add_weight(\n",
        "            name='context_vector', shape=(self.context_vector_length, 1),\n",
        "            initializer=keras.initializers.get('uniform'),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def _get_attention_weights(self, X):\n",
        "        \"\"\"\n",
        "        Computes the attention weights for each timestep in X\n",
        "        :param X: 3d-tensor (batch_size, time_steps, input_dim)\n",
        "        :return: 2d-tensor (batch_size, time_steps) of attention weights\n",
        "        \"\"\"\n",
        "        # Compute a time-wise stimulus, i.e. a stimulus for each\n",
        "        # time step. For this first compute a hidden layer of\n",
        "        # dimension self.context_vector_length and take the\n",
        "        # similarity of this layer with self.u as the stimulus\n",
        "        u_tw = K.tanh(K.dot(X, self.W))\n",
        "        tw_stimulus = K.dot(u_tw, self.u)\n",
        "\n",
        "        # Remove the last axis an apply softmax to the stimulus to\n",
        "        # get a probability.\n",
        "        tw_stimulus = K.reshape(tw_stimulus, (-1, tw_stimulus.shape[1]))\n",
        "        att_weights = K.softmax(tw_stimulus)\n",
        "\n",
        "        return att_weights\n",
        "\n",
        "    def call(self, X):\n",
        "        att_weights = self._get_attention_weights(X)\n",
        "\n",
        "        # Reshape the attention weights to match the dimensions of X\n",
        "        att_weights = K.reshape(att_weights, (-1, att_weights.shape[1], 1))\n",
        "        att_weights = K.repeat_elements(att_weights, X.shape[-1], -1)\n",
        "\n",
        "        # Multiply each input by its attention weights\n",
        "        weighted_input = keras.layers.Multiply()([X, att_weights])\n",
        "\n",
        "        # Sum in the direction of the time-axis.\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[2]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'context_vector_length': self.context_vector_length\n",
        "        }\n",
        "        base_config = super(AttentionLayer, self).get_config()\n",
        "        return {**base_config, **config}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMo2qtZt68Wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the entire HAN model with attention layer and word and setence encoders\n",
        "class HAN(Model):\n",
        "    def __init__(\n",
        "            self, max_words, max_sentences, output_size,\n",
        "            embedding_matrix, word_encoding_dim=200,\n",
        "            sentence_encoding_dim=200, inputs=None,\n",
        "            outputs=None, name='han-for-docla'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A Keras implementation of Hierarchical Attention networks\n",
        "        for document classification.\n",
        "        :param max_words: The maximum number of words per sentence\n",
        "        :param max_sentences: The maximum number of sentences\n",
        "        :param output_size: The dimension of the last layer (i.e.\n",
        "            the number of classes you wish to predict)\n",
        "        :param embedding_matrix: The embedding matrix to use for\n",
        "            representing words\n",
        "        :param word_encoding_dim: The dimension of the GRU\n",
        "            layer in the word encoder.\n",
        "        :param sentence_encoding_dim: The dimension of the GRU\n",
        "            layer in the sentence encoder.\n",
        "        \"\"\"\n",
        "        self.max_words = max_words\n",
        "        self.max_sentences = max_sentences\n",
        "        self.output_size = output_size\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.word_encoding_dim = word_encoding_dim\n",
        "        self.sentence_encoding_dim = sentence_encoding_dim\n",
        "\n",
        "\n",
        "        in_tensor, out_tensor = self._build_network()\n",
        "\n",
        "        super(HAN, self).__init__(\n",
        "            inputs=in_tensor, outputs=out_tensor, name=name\n",
        "        )\n",
        "\n",
        "    def build_word_encoder(self, max_words, embedding_matrix, encoding_dim=200):\n",
        "        \"\"\"\n",
        "        Build the model that embeds and encodes in context the\n",
        "        words used in a sentence. The return model takes a tensor of shape\n",
        "        (batch_size, max_length) that represents a collection of sentences\n",
        "        and returns an encoded representation of these sentences.\n",
        "        :param max_words: (int) The maximum sentence length this model accepts\n",
        "        :param embedding_matrix: (2d array-like) A matrix with the i-th row\n",
        "            representing the embedding of the word represented by index i.\n",
        "        :param encoding_dim: (int, should be even) The dimension of the\n",
        "            bidirectional encoding layer. Half of the nodes are used in the\n",
        "            forward direction and half in the backward direction.\n",
        "        :return: Instance of keras.Model\n",
        "        \"\"\"\n",
        "        assert encoding_dim % 2 == 0, \"Embedding dimension should be even\"\n",
        "\n",
        "        vocabulary_size = embedding_matrix.shape[0]\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "        embedding_layer = Embedding(\n",
        "            vocabulary_size, embedding_dim,\n",
        "            weights=[embedding_matrix], input_length=max_words,\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        sentence_input = Input(shape=(max_words,), dtype='int32')\n",
        "        embedded_sentences = embedding_layer(sentence_input)\n",
        "        encoded_sentences = Bidirectional(\n",
        "            GRU(int(encoding_dim / 2), return_sequences=True)\n",
        "        )(embedded_sentences)\n",
        "\n",
        "        return Model(\n",
        "            inputs=[sentence_input], outputs=[encoded_sentences], name='word_encoder'\n",
        "        )\n",
        "\n",
        "    def build_sentence_encoder(self, max_sentences, summary_dim, encoding_dim=200):\n",
        "        \"\"\"\n",
        "        Build the encoder that encodes the vector representation of\n",
        "        sentences in their context.\n",
        "        :param max_sentences: The maximum number of sentences that can be\n",
        "            passed. Use zero-padding to supply shorter sentences.\n",
        "        :param summary_dim: (int) The dimension of the vectors that summarizes\n",
        "            sentences. Should be equal to the encoding_dim of the word\n",
        "            encoder.\n",
        "        :param encoding_dim: (int, even) The dimension of the vector that\n",
        "            summarizes sentences in context. Half is used in forward direction,\n",
        "            half in backward direction.\n",
        "        :return: Instance of keras.Model\n",
        "        \"\"\"\n",
        "        assert encoding_dim % 2 == 0, \"Embedding dimension should be even\"\n",
        "\n",
        "        text_input = Input(shape=(max_sentences, summary_dim))\n",
        "        encoded_sentences = Bidirectional(\n",
        "            GRU(int(encoding_dim / 2), return_sequences=True)\n",
        "        )(text_input)\n",
        "        return Model(\n",
        "            inputs=[text_input], outputs=[encoded_sentences], name='sentence_encoder'\n",
        "        )\n",
        "\n",
        "    def _build_network(self):\n",
        "        \"\"\"\n",
        "        Build the graph that represents this network\n",
        "        :return: in_tensor, out_tensor, Tensors representing the input and output\n",
        "            of this network.\n",
        "        \"\"\"\n",
        "        in_tensor = Input(shape=(self.max_sentences, self.max_words))\n",
        "\n",
        "        word_encoder = self.build_word_encoder(\n",
        "            self.max_words, self.embedding_matrix, self.word_encoding_dim\n",
        "        )\n",
        "\n",
        "        word_rep = TimeDistributed(\n",
        "            word_encoder, name='word_encoder'\n",
        "        )(in_tensor)\n",
        "\n",
        "        # Sentence Rep is a 3d-tensor (batch_size, max_sentences, word_encoding_dim)\n",
        "        sentence_rep = TimeDistributed(\n",
        "            AttentionLayer(), name='word_attention'\n",
        "        )(word_rep)\n",
        "\n",
        "        doc_rep = self.build_sentence_encoder(\n",
        "            self.max_sentences, self.word_encoding_dim, self.sentence_encoding_dim\n",
        "        )(sentence_rep)\n",
        "\n",
        "        # We get the final representation by applying our attention mechanism\n",
        "        # to the encoded sentences\n",
        "        doc_summary = AttentionLayer(name='sentence_attention')(doc_rep)\n",
        "\n",
        "        out_tensor = Dense(\n",
        "            self.output_size, activation='softmax', name='class_prediction'\n",
        "        )(doc_summary)\n",
        "\n",
        "        return in_tensor, out_tensor\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'max_words': self.max_words,\n",
        "            'max_sentences': self.max_sentences,\n",
        "            'output_size': self.output_size,\n",
        "            'embedding_matrix': self.embedding_matrix,\n",
        "            'word_encoding_dim': self.word_encoding_dim,\n",
        "            'sentence_encoding_dim': self.sentence_encoding_dim,\n",
        "            'base_config': super(HAN, self).get_config()\n",
        "        }\n",
        "\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, custom_objects=None):\n",
        "        \"\"\"\n",
        "        Keras' API isn't really extendible at this point\n",
        "        therefore we need to use a bit hacky solution to\n",
        "        be able to correctly reconstruct the HAN model\n",
        "        from a config. This therefore does not reconstruct\n",
        "        a instance of HAN model, but actually a standard\n",
        "        Keras model that behaves exactly the same.\n",
        "        \"\"\"\n",
        "        base_config = config.pop('base_config')\n",
        "\n",
        "        return Model.from_config(\n",
        "            base_config, custom_objects=custom_objects\n",
        "        )\n",
        "\n",
        "    def predict_sentence_attention(self, X):\n",
        "        \"\"\"\n",
        "        For a given set of texts predict the attention\n",
        "        weights for each sentence.\n",
        "        :param X: 3d-tensor, similar to the input for predict\n",
        "        :return: 2d array (num_obs, max_sentences) containing\n",
        "            the attention weights for each sentence\n",
        "        \"\"\"\n",
        "        att_layer = self.get_layer('sentence_attention')\n",
        "        prev_tensor = att_layer.input\n",
        "\n",
        "        # Create a temporary dummy layer to hold the\n",
        "        # attention weights tensor\n",
        "        dummy_layer = Lambda(\n",
        "            lambda x: att_layer._get_attention_weights(x)\n",
        "        )(prev_tensor)\n",
        "\n",
        "        return Model(self.input, dummy_layer).predict(X)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VhJKuTZ84qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# deleting the glove model as we have created out embedding matrix\n",
        "del model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-duQD5wA7C4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9e298e17-c0d4-4f16-ef8a-461f12e89b2a"
      },
      "source": [
        "han_model = HAN(\n",
        "    MAX_WORDS_PER_SENT, MAX_SENT, 2, embedding_matrix,\n",
        "    word_encoding_dim=100, sentence_encoding_dim=100\n",
        ")\n",
        "han_model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"han-for-docla\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 40, 50)            0         \n",
            "_________________________________________________________________\n",
            "word_encoder (TimeDistribute (None, 40, 50, 100)       53978580  \n",
            "_________________________________________________________________\n",
            "word_attention (TimeDistribu (None, 40, 100)           10100     \n",
            "_________________________________________________________________\n",
            "sentence_encoder (Model)     (None, 40, 100)           45300     \n",
            "_________________________________________________________________\n",
            "sentence_attention (Attentio (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "class_prediction (Dense)     (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 54,044,282\n",
            "Trainable params: 135,002\n",
            "Non-trainable params: 53,909,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6je8v_b27PZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "han_model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy',\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auhYMRFu7ktR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a checkpoint to save model after every epoch if val loss decreases\n",
        "checkpoint_saver = ModelCheckpoint(\n",
        "    filepath='HAN_Model.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "    verbose=1, save_best_only=True\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqvbz_mJ7YRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training the model\n",
        "han_model.fit(\n",
        "    X_train, Y_train, batch_size=20, epochs=5,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    callbacks=[checkpoint_saver]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LyckcKnkJff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining a function which calculates various metrics such as micro and macro precision, accuracy and f1\n",
        "def metrics_calculator(preds, test_labels):\n",
        "    cm = confusion_matrix(test_labels, preds)\n",
        "    TP = []\n",
        "    FP = []\n",
        "    FN = []\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[i][j]\n",
        "\n",
        "        FN.append(summ)\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[j][i]\n",
        "\n",
        "        FP.append(summ)\n",
        "    for i in range(0,2):\n",
        "        TP.append(cm[i][i])\n",
        "    precision = []\n",
        "    recall = []\n",
        "    for i in range(0,2):\n",
        "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
        "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
        "\n",
        "    macro_precision = sum(precision)/2\n",
        "    macro_recall = sum(recall)/2\n",
        "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
        "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
        "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
        "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
        "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8_5rhKkkxUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the predicted labels on the test data\n",
        "predict = han_model.predict(X_test, batch_size = 20)\n",
        "y_pred = predict > 0.5\n",
        "y = []\n",
        "for i in range(len(y_pred)):\n",
        "  y.append(int(y_pred[i][1]))\n",
        "\n",
        "# Calculating all metrics on test data predicted label\n",
        "print(metrics_calculator(y, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOo7ri4pnszY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the predicted labels on the dev data\n",
        "preds = han_model.predict(X_val, batch_size= 20)\n",
        "y_pred_dev = preds > 0.5\n",
        "y = []\n",
        "for i in range(len(y_pred_dev)):\n",
        "  y.append(int(y_pred_dev[i][1]))\n",
        "\n",
        "# Calculating all metrics on dev data predicted label\n",
        "print(metrics_calculator(y, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC3VWHektoAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the final HAN Model\n",
        "han_model.save('HAN_final.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}