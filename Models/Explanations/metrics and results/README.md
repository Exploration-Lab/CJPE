# File Description

**File Name**| **Description**
---------|------------
**occ_explanations.json**| This json file contains the explanatory sentences collected using the algrithm in the paper. You can access the explanations by using the the file name of any file in **ILDC<sub>exper</sub>**
**gold_explanations.json**| This json file contains the explanatory sentences collected from the users' annotation. To access the annotations you need to give the file name of any file in **ILDC<sub>expert</sub>**, then the user number like this "User 2" (number should be between 1 to 5) and then finally "exp" if you want the explanation or "verdict" if you want to access the verdict of judgment given by the user (Note that this is not the true label).
**gold_explanations_ranked.json**| This json file contains the explanatory sentences collected from the users' annotation. The difference between this file and above file is that in this file you can access the sentences belonging to a particular rank as well (Rank 1 to 10). So, to access the explanations you need to give the file name of any file in **ILDC<sub>expert</sub>**, then the user number like this "User 2" (number should be between 1 to 5) and then the rank as "Rank4", finally "exp" if you want the explanation or "verdict" if you want to access the verdict of judgment given by the user.
**anno_explanations_scores.xlsx**| This is an xlsx file having 56 sheets for each of the file in **ILDC<sub>expert</sub>**, and using this one can refer to scores of the metrics (like ROUGE-1, ROUGE-2, ROUGE-l, Jaccard Similarity, Overlap-min, Overlap-max, METEOR and BLEU) for every single file. Each sheet contains the "all ranks combined explanations" and verdit as well.
**xl_anno_make.py**| This is the script for generating anno_explanations_scores.xlsx
**metricmaker.py**| To reproduce machine vs. user explanations results, use this file. You will need the files **gold_explanations_ranked.json** and **occ_explanations.json** to generated results. You can manipulate the code according to what collections of ranks (e.g. Ranks 1 to 5) you want to compare the machine explanations with. Currently, it has been set to generate the combinations: Ranks 1 to 10, Ranks 1 only, Rank 2 only, Rank 3 only, Rank 4 only, Rank 5 only, Rank 6 only, Rank 7 only, Rank 8 only, Rank 9 only, Rank 10 only, Ranks 1 to 5, Ranks 5 to 10. The results files will be generated in the folder result_files.
**result_files**| This folder contains the results generated from metricmaker.py
**among user scores**| This folder contains the scores averaged across all 56 files of each metric type (ROUGE-1, ROUGE-2, ROUGE-l, Jaccard Similarity, Overlap-min, Overlap-max, METEOR and BLEU) for every user from 1 to 5
