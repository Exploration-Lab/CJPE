{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VE_XLNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMVW_3xS5_zF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "9b9fe623-7c5e-40bf-f65b-b7823c36ce29"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |▌                               | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=fd1e525d1b26a9bbe067d5d7cc62f96a125a53124325fac0f50434bd2bef288b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4t7lOMh6KEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bab15da8-0ec4-4a7b-9720-8019bc5eac5e"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import textwrap\n",
        "import progressbar\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6nfznji6OTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n",
        "\n",
        "model_type = 'xlnet' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n",
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
        "model_name = 'xlnet-base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lccO0FC7C70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "54067e5f-9d5c-421d-8c78-c261afbe3e2f"
      },
      "source": [
        "drive.mount(\"/content/Drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdZLCYyS7N2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecdabb30-78ce-4c81-a12d-d39f0b00bd10"
      },
      "source": [
        "output_dir = \"/content/Drive/My Drive/XLNet_right_model/\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = XLNetForSequenceClassification.from_pretrained(output_dir, output_hidden_states=True)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AXyVhee7i3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/Drive/My Drive/LNLP/dataset.csv')\n",
        "train_set = df.query(\" split=='train' \")\n",
        "test_set = df.query(\" split=='test' \")\n",
        "validation_set = df.query(\" split=='dev' \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwVO9zry7l06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def att_masking(input_ids):\n",
        "  attention_masks = []\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n",
        "  return attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjTzNEQb7oAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grouped_input_ids(all_toks):\n",
        "  splitted_toks = []\n",
        "  l=0\n",
        "  r=510\n",
        "  while(l<len(all_toks)):\n",
        "    splitted_toks.append(all_toks[l:min(r,len(all_toks))])\n",
        "    l+=410\n",
        "    r+=410\n",
        "\n",
        "  CLS = tokenizer.cls_token\n",
        "  SEP = tokenizer.sep_token\n",
        "  e_sents = []\n",
        "  for l_t in splitted_toks:\n",
        "    l_t = l_t + [SEP] + [CLS]\n",
        "    encoded_sent = tokenizer.convert_tokens_to_ids(l_t)\n",
        "    e_sents.append(encoded_sent)\n",
        "\n",
        "  e_sents = pad_sequences(e_sents, maxlen=512, value=0, dtype=\"long\", padding=\"pre\")\n",
        "  att_masks = att_masking(e_sents)\n",
        "  return e_sents, att_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUIXwsRg8JDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output_for_one_vec(input_id, att_mask):\n",
        "  input_ids = torch.tensor(input_id)\n",
        "  att_masks = torch.tensor(att_mask)\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  att_masks = att_masks.unsqueeze(0)\n",
        "  model.eval()\n",
        "  input_ids = input_ids.to(device)\n",
        "  att_masks = att_masks.to(device)\n",
        "  with torch.no_grad():\n",
        "      logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
        "\n",
        "  #vec = encoded_layers[12][0][0]\n",
        "  #vec = vec.detach().cpu().numpy()\n",
        "  #return vec\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57eQt9KY-ZuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_np_files_for_emb(dataf, tokenizer):\n",
        "  all_docs = []\n",
        "  ####\n",
        "  my_preds = []\n",
        "  ####\n",
        "  for i in progressbar.progressbar(range(len(dataf['text']))):\n",
        "    text = dataf['text'].iloc[i]\n",
        "    toks = tokenizer.tokenize(text)\n",
        "    if(len(toks) > 10000):\n",
        "      toks = toks[len(toks)-10000:]\n",
        "\n",
        "    splitted_input_ids, splitted_att_masks = grouped_input_ids(toks)\n",
        "\n",
        "    vecs = []\n",
        "    for index,ii in enumerate(splitted_input_ids):\n",
        "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
        " \n",
        "    lval = 0.0\n",
        "    for i in vecs:\n",
        "      lval+=i\n",
        "\n",
        "    if(lval/len(vecs) > 0.5):\n",
        "      my_preds.append(1)\n",
        "    else:\n",
        "      my_preds.append(0)\n",
        "\n",
        "  return my_preds\n",
        "  #   one_doc = np.asarray(vecs)\n",
        "  #   all_docs.append(one_doc)\n",
        "\n",
        "  # all_docs = np.asarray(all_docs)\n",
        "  # return all_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfSy1Vou-pNE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "759401ec-3a56-4a31-d130-78f408e69408"
      },
      "source": [
        "preds_dev = generate_np_files_for_emb(validation_set, tokenizer)\n",
        "dev_labels = validation_set['label'].to_list()\n",
        "print(len(preds_dev))\n",
        "correct=0\n",
        "for index in range(len(preds_dev)):\n",
        "  if(preds_dev[index] == dev_labels[index]):\n",
        "    correct+=1\n",
        "\n",
        "print(correct/len(preds_dev))\n",
        "#np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/BERT/BERT_dev.npy\", vecs_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (994 of 994) |######################| Elapsed Time: 0:06:47 Time:  0:06:47\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "994\n",
            "0.6177062374245473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bql-xSHX-5j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import textwrap\n",
        "import progressbar\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def metrics_calculator(preds, test_labels):\n",
        "    cm = confusion_matrix(test_labels, preds)\n",
        "    TP = []\n",
        "    FP = []\n",
        "    FN = []\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[i][j]\n",
        "\n",
        "        FN.append(summ)\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[j][i]\n",
        "\n",
        "        FP.append(summ)\n",
        "    for i in range(0,2):\n",
        "        TP.append(cm[i][i])\n",
        "    precision = []\n",
        "    recall = []\n",
        "    for i in range(0,2):\n",
        "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
        "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
        "\n",
        "    macro_precision = sum(precision)/2\n",
        "    macro_recall = sum(recall)/2\n",
        "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
        "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
        "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
        "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
        "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jttK9vTF_Chp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0bd4846b-24ce-4ee7-d78b-5b7ce83ea327"
      },
      "source": [
        "metrics_calculator(preds_dev, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7085314685314685,\n",
              " 0.6177062374245472,\n",
              " 0.6600088439017393,\n",
              " 0.6177062374245473,\n",
              " 0.6177062374245473,\n",
              " 0.6177062374245473)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcYi__ae_IJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9e82f279-19be-4094-c055-6e4711b653b3"
      },
      "source": [
        "preds_test = generate_np_files_for_emb(test_set, tokenizer)\n",
        "test_labels = test_set['label'].to_list()\n",
        "print(len(preds_test))\n",
        "correct=0\n",
        "for index in range(len(preds_test)):\n",
        "  if(preds_test[index] == test_labels[index]):\n",
        "    correct+=1\n",
        "\n",
        "print(correct/len(preds_test))\n",
        "#np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/BERT/BERT_dev.npy\", vecs_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (1517 of 1517) |####################| Elapsed Time: 0:10:36 Time:  0:10:36\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1517\n",
            "0.5992089650626236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNhNg7dOdpJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6350343-0c9c-4163-a96f-85f681dfddc0"
      },
      "source": [
        "metrics_calculator(preds_test, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6784146394025035,\n",
              " 0.6007335175818255,\n",
              " 0.6372153381639349,\n",
              " 0.5992089650626236,\n",
              " 0.5992089650626236,\n",
              " 0.5992089650626236)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYBltKw0drSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}