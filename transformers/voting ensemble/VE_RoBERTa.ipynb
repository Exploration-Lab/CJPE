{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VE_RoBERTa.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pnbgWAg0Esk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "3f8e62dd-2e64-4c3b-960e-af6f2ecba381"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 30.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 36.6MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 36.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 19.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 18.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 15.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 15.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 15.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0002c4671854c866147c96b627e223843d38f8dee62633fed64e9f39b52c9cff\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8o2rH-b0G9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee2c899f-a0c4-4695-b4d8-6e4083bd47d9"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import textwrap\n",
        "import progressbar\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDuFrSXy0NTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n",
        "\n",
        "model_type = 'roberta' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n",
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
        "model_name = 'roberta-base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK3cl9xg0RT2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7de7dc9b-4411-45a7-f426-10766d877f5d"
      },
      "source": [
        "drive.mount(\"/content/Drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpzz4KwH0Ug1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "422d2827-d51d-46bd-e5dc-51ec8277882f"
      },
      "source": [
        "output_dir = \"/content/Drive/My Drive/RoBERTa_right_model2/\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RobertaForSequenceClassification.from_pretrained(output_dir, output_hidden_states=True)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glqARANz0ox3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/Drive/My Drive/LNLP/dataset.csv')\n",
        "train_set = df.query(\" split=='train' \")\n",
        "test_set = df.query(\" split=='test' \")\n",
        "validation_set = df.query(\" split=='dev' \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEgqok1Z1do9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def att_masking(input_ids):\n",
        "  attention_masks = []\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n",
        "  return attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPd82jjA1gQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grouped_input_ids(all_toks):\n",
        "  splitted_toks = []\n",
        "  l=0\n",
        "  r=510\n",
        "  while(l<len(all_toks)):\n",
        "    splitted_toks.append(all_toks[l:min(r,len(all_toks))])\n",
        "    l+=410\n",
        "    r+=410\n",
        "\n",
        "  CLS = tokenizer.cls_token\n",
        "  SEP = tokenizer.sep_token\n",
        "  e_sents = []\n",
        "  for l_t in splitted_toks:\n",
        "    l_t = [CLS] + l_t + [SEP]\n",
        "    encoded_sent = tokenizer.convert_tokens_to_ids(l_t)\n",
        "    e_sents.append(encoded_sent)\n",
        "\n",
        "  e_sents = pad_sequences(e_sents, maxlen=512, value=0, dtype=\"long\", padding=\"post\")\n",
        "  att_masks = att_masking(e_sents)\n",
        "  return e_sents, att_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5nKXsET1qjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_output_for_one_vec(input_id, att_mask):\n",
        "  input_ids = torch.tensor(input_id)\n",
        "  att_masks = torch.tensor(att_mask)\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  att_masks = att_masks.unsqueeze(0)\n",
        "  model.eval()\n",
        "  input_ids = input_ids.to(device)\n",
        "  att_masks = att_masks.to(device)\n",
        "  with torch.no_grad():\n",
        "      logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
        "\n",
        "  #vec = encoded_layers[12][0][0]\n",
        "  #vec = vec.detach().cpu().numpy()\n",
        "  #return vec\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbDhPIex1q6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_np_files_for_emb(dataf, tokenizer):\n",
        "  all_docs = []\n",
        "  ####\n",
        "  my_preds = []\n",
        "  ####\n",
        "  for i in progressbar.progressbar(range(len(dataf['text']))):\n",
        "    text = dataf['text'].iloc[i]\n",
        "    toks = tokenizer.tokenize(text, add_prefix_space=True)\n",
        "    if(len(toks) > 10000):\n",
        "      toks = toks[len(toks)-10000:]\n",
        "\n",
        "    splitted_input_ids, splitted_att_masks = grouped_input_ids(toks)\n",
        "\n",
        "    vecs = []\n",
        "    for index,ii in enumerate(splitted_input_ids):\n",
        "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
        " \n",
        "    lval = 0.0\n",
        "    for i in vecs:\n",
        "      lval+=i\n",
        "\n",
        "    if(lval/len(vecs) > 0.5):\n",
        "      my_preds.append(1)\n",
        "    else:\n",
        "      my_preds.append(0)\n",
        "\n",
        "  return my_preds\n",
        "  #   one_doc = np.asarray(vecs)\n",
        "  #   all_docs.append(one_doc)\n",
        "\n",
        "  # all_docs = np.asarray(all_docs)\n",
        "  # return all_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOA2nYpA1s2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7a594fd1-5c03-4679-9a50-7d90c5778f10"
      },
      "source": [
        "preds_dev = generate_np_files_for_emb(validation_set, tokenizer)\n",
        "dev_labels = validation_set['label'].to_list()\n",
        "print(len(preds_dev))\n",
        "correct=0\n",
        "for index in range(len(preds_dev)):\n",
        "  if(preds_dev[index] == dev_labels[index]):\n",
        "    correct+=1\n",
        "\n",
        "print(correct/len(preds_dev))\n",
        "#np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/BERT/BERT_dev.npy\", vecs_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (994 of 994) |######################| Elapsed Time: 0:08:23 Time:  0:08:23\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "994\n",
            "0.6348088531187123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNmKBcy121Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import textwrap\n",
        "import progressbar\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def metrics_calculator(preds, test_labels):\n",
        "    cm = confusion_matrix(test_labels, preds)\n",
        "    TP = []\n",
        "    FP = []\n",
        "    FN = []\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[i][j]\n",
        "\n",
        "        FN.append(summ)\n",
        "    for i in range(0,2):\n",
        "        summ = 0\n",
        "        for j in range(0,2):\n",
        "            if(i!=j):\n",
        "                summ=summ+cm[j][i]\n",
        "\n",
        "        FP.append(summ)\n",
        "    for i in range(0,2):\n",
        "        TP.append(cm[i][i])\n",
        "    precision = []\n",
        "    recall = []\n",
        "    for i in range(0,2):\n",
        "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
        "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
        "\n",
        "    macro_precision = sum(precision)/2\n",
        "    macro_recall = sum(recall)/2\n",
        "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
        "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
        "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
        "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
        "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AglmHRF275G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a10bb085-8797-4a66-8bd7-0ff57924cab2"
      },
      "source": [
        "metrics_calculator(preds_dev, dev_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6942946832843206,\n",
              " 0.6348088531187123,\n",
              " 0.663220583725073,\n",
              " 0.6348088531187123,\n",
              " 0.6348088531187123,\n",
              " 0.6348088531187123)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOGAHwMr3PSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e4817d9f-39f7-4cd0-96cc-079decaf5d1e"
      },
      "source": [
        "preds_test = generate_np_files_for_emb(test_set, tokenizer)\n",
        "test_labels = test_set['label'].to_list()\n",
        "print(len(preds_test))\n",
        "correct=0\n",
        "for index in range(len(preds_test)):\n",
        "  if(preds_test[index] == test_labels[index]):\n",
        "    correct+=1\n",
        "\n",
        "print(correct/len(preds_test))\n",
        "#np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/BERT/BERT_dev.npy\", vecs_dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (1517 of 1517) |####################| Elapsed Time: 0:13:01 Time:  0:13:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1517\n",
            "0.6242584047462096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4tdQ7mt3Yyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "37c5be83-c236-4970-ea95-2e698bef8460"
      },
      "source": [
        "metrics_calculator(preds_test, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6820188702068465,\n",
              " 0.6255462272513949,\n",
              " 0.6525630456204138,\n",
              " 0.6242584047462096,\n",
              " 0.6242584047462096,\n",
              " 0.6242584047462096)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw46Jdw44dYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}