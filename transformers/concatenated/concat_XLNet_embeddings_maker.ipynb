{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mG41G9dM8z6L",
    "outputId": "d161f77b-5819-4501-aba3-86447c2888b4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "import textwrap\n",
    "import progressbar\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOQ2YDd-87WS"
   },
   "outputs": [],
   "source": [
    "drive.mount(\"/content/Drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyEd_uKF8-Dj"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/Drive/My Drive/LNLP/dataset.csv') # path to multi dataset\n",
    "train_set = df.query(\" split=='train' \")\n",
    "test_set = df.query(\" split=='test' \")\n",
    "validation_set = df.query(\" split=='dev' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEieUV-l9MIj"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
    "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
    "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n",
    "\n",
    "model_type = 'xlnet' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n",
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
    "model_name = 'xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gHCyZeBm9OnR",
    "outputId": "09e8b5cd-a64d-4e6b-d28a-be2425c5e2e5"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = \"/content/Drive/My Drive/mini_XLNet/\"\n",
    "tokenizer = XLNetTokenizer.from_pretrained(output_dir)\n",
    "model = XLNetForSequenceClassification.from_pretrained(output_dir, output_hidden_states=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2swrOWVF-MKu"
   },
   "outputs": [],
   "source": [
    "def att_masking(input_ids):\n",
    "  attention_masks = []\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)\n",
    "  return attention_masks\n",
    "\n",
    "def grouped_input_ids(all_toks):\n",
    "  splitted_toks = []\n",
    "  l=0\n",
    "  r=510\n",
    "  while(l<len(all_toks)):\n",
    "    splitted_toks.append(all_toks[l:min(r,len(all_toks))])\n",
    "    l+=410\n",
    "    r+=410\n",
    "\n",
    "  CLS = tokenizer.cls_token\n",
    "  SEP = tokenizer.sep_token\n",
    "  e_sents = []\n",
    "  for l_t in splitted_toks:\n",
    "    l_t = l_t + [SEP] + [CLS]\n",
    "    encoded_sent = tokenizer.convert_tokens_to_ids(l_t)\n",
    "    e_sents.append(encoded_sent)\n",
    "\n",
    "  e_sents = pad_sequences(e_sents, maxlen=512, value=0, dtype=\"long\", padding=\"pre\")\n",
    "  att_masks = att_masking(e_sents)\n",
    "  return e_sents, att_masks\n",
    "\n",
    "def get_output_for_one_vec(input_id, att_mask):\n",
    "  input_ids = torch.tensor(input_id)\n",
    "  att_masks = torch.tensor(att_mask)\n",
    "  input_ids = input_ids.unsqueeze(0)\n",
    "  att_masks = att_masks.unsqueeze(0)\n",
    "  model.eval()\n",
    "  input_ids = input_ids.to(device)\n",
    "  att_masks = att_masks.to(device)\n",
    "  with torch.no_grad():\n",
    "      logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "\n",
    "  vec = torch.cat((encoded_layers[12][0][-1], encoded_layers[11][0][-1], encoded_layers[10][0][-1], encoded_layers[9][0][-1]), dim=0)\n",
    "  vec = vec.detach().cpu().numpy()\n",
    "  return vec\n",
    "\n",
    "def generate_np_files_for_emb(dataf, tokenizer):\n",
    "  all_docs = []\n",
    "  for i in progressbar.progressbar(range(len(dataf['text']))):\n",
    "    text = dataf['text'].iloc[i]\n",
    "    toks = tokenizer.tokenize(text, add_prefix_space=True)\n",
    "    if(len(toks) > 10000):\n",
    "      toks = toks[len(toks)-10000:]\n",
    "\n",
    "    splitted_input_ids, splitted_att_masks = grouped_input_ids(toks)\n",
    "\n",
    "    vecs = []\n",
    "    for index,ii in enumerate(splitted_input_ids):\n",
    "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
    " \n",
    "    one_doc = np.asarray(vecs)\n",
    "    all_docs.append(one_doc)\n",
    "  \n",
    "\n",
    "  all_docs = np.asarray(all_docs)\n",
    "  return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mmuAemf_j5y"
   },
   "outputs": [],
   "source": [
    "vecs_dev = generate_np_files_for_emb(validation_set, tokenizer)\n",
    "np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/XLNet_full_concat/XLNet_dev.npy\", vecs_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Evhxb371_qwb"
   },
   "outputs": [],
   "source": [
    "train_set_1 = train_set.iloc[:10000]\n",
    "train_set_2 = train_set.iloc[10000:20000]\n",
    "train_set_3 = train_set.iloc[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GgEPrgz9B7Pr",
    "outputId": "2281d57f-9af1-4070-9f29-f54ac5bc8467"
   },
   "outputs": [],
   "source": [
    "vecs_test = generate_np_files_for_emb(test_set, tokenizer)\n",
    "np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/XLNet_full_concat/XLNet_test.npy\", vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tTDe7bmlB-8O",
    "outputId": "aba8532b-0846-43a5-8662-76768e965dcf"
   },
   "outputs": [],
   "source": [
    "vecs_train_1 = generate_np_files_for_emb(train_set_1, tokenizer)\n",
    "np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/XLNet_full_concat/XLNet_train_1.npy\", vecs_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7ep0_HMqUCXw",
    "outputId": "aab9d0aa-ead6-4806-a463-f8628caa97ee"
   },
   "outputs": [],
   "source": [
    "vecs_train_2 = generate_np_files_for_emb(train_set_2, tokenizer)\n",
    "np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/XLNet_full_concat/XLNet_train_2.npy\", vecs_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PQDfbm5LUCi8",
    "outputId": "d2eebab5-c446-4d0c-a0a3-7a8dde125ec1"
   },
   "outputs": [],
   "source": [
    "vecs_train_3 = generate_np_files_for_emb(train_set_3, tokenizer)\n",
    "np.save(\"/content/Drive/My Drive/LNLP/Hierarchical/XLNet_full_concat/XLNet_train_3.npy\", vecs_train_3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "concat_XLNet_embeddings_maker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
